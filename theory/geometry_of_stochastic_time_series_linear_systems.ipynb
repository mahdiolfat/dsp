{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guassian Random Variable\n",
    "\n",
    "## Stochastic Systems\n",
    "\n",
    "## Wiener\n",
    "\n",
    "## Linear Representation of Gaussian Stochastic Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Golden Result: Maximum Entropy (still linear tho!)\n",
    "\n",
    "This is one of the first results that bridged information theory controls engineering, more specifically the topic of optimal. There is in fact a \"sad\" tale of tales that Wiener was quite upset with \"shannon\" for publishing what he thought he had already explored and published (i.e., Optimal Wiener Filtering). However, there is no doubt that he did not publish any derivations connecting the probabilistic measure of entropy / uncertainty / information with his optimal linear filters\n",
    "\n",
    "A real-world system is typically sampled uniformely in time by an Analog-to-Digital Converter producing digital results at a rate of Fs. In order to treat the data statistically, a set of time-series data (assumed to be sampled uniformely in time at T = 1 / fs) with a prior determinist signal in white gaussian noise (for example, a single sinusoid in white gaussian noise where the sinusoid amplitude is substantially greater than the variance of the white noise it is measured in. This assumption is equivalent to stating that the measured is reasonbly information preserving and matches the representation of the model (continuous-time  - aka analog signal, finite-dimensional - aka band-limited - system , aka wide sense stationary, with no dynamics past some system bandwidth,in this case, the uniformly sampled time series is an analog bandlimited signal observed over T seconds long).\n",
    "\n",
    "It is perhaps one of the most standard technique used to this day is called \"preprocess\" / \"wrangle\", or \"whitening\" step. It refers to a binary split in signal energy such that all correlated samples accumulate in the parameters of the model (e.g., deterministic part of the measured noisy signal) and the rest is evenly distributed as noise. This tends to flatten the power spectrum to a constant \"noise floor\". This binary split in signal energy may also be though of as a perfect factorization of the spectral density function of the measured signal (again, assuming only additive white gaussian noise and the determinist system as expected)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
