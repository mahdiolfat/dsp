{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terms\n",
    "\n",
    "$$ MATRIX() $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measurable Signal Properties\n",
    "\n",
    "## Bounded Variation\n",
    "\n",
    "# Classes of Signals: Functions in Spaces\n",
    "\n",
    "# Geometric Signal Processing\n",
    "\n",
    "A generalization of all finite energy / bounded power signals. May be linear or not. Any non-zero function that does not blow up to infinity may be defined this way.\n",
    "\n",
    "All \"classical\" representation of signals can be interpreted as some representation of an inner product and projection to a non-zero subspace with normalized basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Borel, Banach, and Hilbert Spaces\n",
    "\n",
    "Note: see REF for a formal mathematical treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Theory\n",
    "\n",
    "The main reference for this notebook is the published monogram \"Information Geometry and Its Application\" written by the founder and contemparory key figures of the field: Shun-ichi Amari.\n",
    "\n",
    "## Mathematical Domain\n",
    "\n",
    "Statistical Inference\n",
    "\n",
    "Differential Geometry is a pre-requisite for a true mathematical understanding, however, since there won't be much proof in this notebook, relevent aspects will be mentioned as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Historical Background\n",
    "\n",
    "As put succinctly by Amari, \"Information Geometry has emerged from a study of invariant geometrical structures of regular statistical models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Main Definitions\n",
    "\n",
    "The main mathematical concept of a \"structure\" in higher-dimensional (more than 1 dimension) is a manifold, which in this case needs to meet basic differentiability and convergence conditions around the point (the points neighborhood of values). Since such manifold are assumed throughout the whole notebook, the rigorous details are omitted. A manifold is a Hausdroff space which is covered by a number of open sets called coordinate neghbourhoods, this coordinate neghborhoods may be transformed (an isomorphism) to a well structured Euclidian space of some dimension d $$ R^{d} $$.\n",
    "\n",
    "Simply put, we may assume the isomorphism exists and we can treat any local coordinate neighborhood of any type of manifold having a Euclidian structure. Coordinate neighborhood and coordinate transformation are assumed to be differentiable.\n",
    "\n",
    "### Positive Definite Matrices\n",
    "\n",
    "Properties:\n",
    "\n",
    "Manifolds of \n",
    "\n",
    "\n",
    "\n",
    "### Convex Functions\n",
    "\n",
    "Sufficient Properties of a Convex Function\n",
    "\n",
    "Obviously, a convex function cannot be a linear function, so that is the easiest \"eye\" test. But for a nonlinear function with no other restriction, the following inequality is sufficient:\n",
    "\n",
    "For a convex function to be a differentiable convex function, it's Hessian must exist and be positive definite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Maniforld of Probability Distributions\n",
    "\n",
    "#### Exponential Family of Probability Distributions\n",
    "\n",
    "The PDF is given by:\n",
    "\n",
    "$$\n",
    "p(x, \\theta) = \\exp \\left\\lbrace\\sum \\theta_i x_i + k(x) - \\psi(\\theta) \\right\\rbrace dx\n",
    "$$\n",
    "\n",
    "$ p(x, \\theta) $ is the probability density function of vector random variable $ x $ specified by vector parameter $ \\theta $. $ k(x) is a function of $ x $. The term \\exp \\lbrace \\psi(\\theta) \\rbrace $ is the normalization factor with which the requirement of the PDF is satisified, i.e.,\n",
    "\n",
    "$$\n",
    "\\int p(x, \\theta)dx = 1\n",
    "$$\n",
    "\n",
    "$ \\psi(\\theta) $ is given by\n",
    "\n",
    "$$\n",
    "\\psi(\\theta) = \\log \\int \\exp \\left\\lbrace \\sum \\theta_i x_i + k(x) \\right \\rbrace\n",
    "$$\n",
    "\n",
    "$ \\psi(\\theta) $ is a convex function. It is known as the cumulant generating function in statistics and free energy in statistical physics.\n",
    "\n",
    "To prove that it is convex, consider that any nonlinear function $ \\psi(\\theta) $ is said to be convex when the following inequality holds:\n",
    "\n",
    "$$\n",
    "\\lambda \\psi(\\xi_1) + (1 - \\lambda)\\psi(\\xi_2) \\geq \\psi \\left\\lbrace \\lambda \\xi_1 + (1 - \\lambda) \\psi_2 \\right\\rbrace\n",
    "$$\n",
    "\n",
    "for any $ \\xi_1 $, $ \\xi_2 $, and scalar $ 0 \\leq \\lambda \\leq 1 $.\n",
    "\n",
    "#### Gaussian Distributions\n",
    "\n",
    "#### Discrete Distributions\n",
    "\n",
    "#### \"Regular\" Statistical Model\n",
    "\n",
    "### Riemannian Metric\n",
    "\n",
    "## Theorems\n",
    "\n",
    "## Geometry of Systems\n",
    "\n",
    "## Applications\n",
    "\n",
    "### PCA and ICA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latest Research\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Lemmas\n",
    "\n",
    "## Algorithms and Implementations\n",
    "\n",
    "## Related Terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defs = {\n",
    "    \"Neyman-Scott Problem\":\n",
    "        [\"An auto-regressive model is a time series generated from white noise by\"],\n",
    "    \"Semiparametrics\": [],\n",
    "    \"nuisance parameter\": [],\n",
    "    \"estimating function\": [],\n",
    "    \"score function\": [],\n",
    "    \"loss function\": [],\n",
    "    \"Orthogonal Matching Pursuit (OMP)\": [\n",
    "        \"OMP iteratively select the vectors that contain most of the energy of the measurement vector.\",\n",
    "        \"The selection at each iteration is made based on intter products between the model columns and a residual.\",\n",
    "        \" The residual reflects the component of the measurement vector that is orthogonal to the previously selected columns.\",\n",
    "        \"OMP is guaranteed to converg within a finite number of iterations.\"\n",
    "    ],\n",
    "    \"Tree Matching Pursuit (TMP)\"\n",
    "    \"AR Model\": [],\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
